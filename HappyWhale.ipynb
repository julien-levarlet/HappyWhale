{"cells":[{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3119,"status":"ok","timestamp":1650141231169,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"wGlyYvluyNcx","outputId":"802b5c24-8b87-4664-d6b1-61ce7f19a573"},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["try:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  !mkdir src\n","  !cp -r drive/MyDrive/IFT780/HappyWhale/src/* src/\n","  !mkdir data\n","  !cp -r drive/MyDrive/IFT780/HappyWhale/data/* data/\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","  #%pip install -r requirements.txt\n","\n","# Pour automatiquement recharger les modules externes\n","# voir http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"XTgcMFrryNc7"},"source":["# HappyWhale Challenge - Team WhalePlayed\n","\n","This notebook presents the work done by Gaétan Rey, Julien Levarlet and Timothée Wright, as part of the challenge https://www.kaggle.com/competitions/happy-whale-and-dolphin/overview ."]},{"cell_type":"markdown","metadata":{"id":"FBEpvY25yNc-"},"source":["Imports :"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":2501,"status":"ok","timestamp":1650141233661,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"IF1wvX3dyNc_"},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","from src.ModelTrainTestManager import ModelTrainTestManager, optimizer_setup\n","from src.DataManager import DataManager\n","from src.Models.ResNet import ResNet\n","from src.Models.ArcFace import HappyWhaleModel"]},{"cell_type":"markdown","metadata":{"id":"HPLcrHV6yNdA"},"source":["Parameters for the data :"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1650141233664,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"NHnVDFYkyNdB"},"outputs":[],"source":["data_csv = \"data/common_train.csv\"\n","dataFolderPath = \"data/common_cropped_train_imgs\""]},{"cell_type":"markdown","metadata":{"id":"HsWIuBp7yNdC"},"source":["Parameters for the training :"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1650141233666,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"oE-WSe41yNdD"},"outputs":[],"source":["batch_size = 10\n","learning_rate = 0.01\n","optimizer_factory = optimizer_setup(torch.optim.Adam, lr=learning_rate)\n","\n","test_percentage = 0.2\n","val_percentage = 0.2\n","\n","exp_name = \"HappyWhale\""]},{"cell_type":"markdown","metadata":{"id":"VvLH9ra-yNdF"},"source":["Parameters for the model :"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":206,"status":"ok","timestamp":1650141233853,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"cHz9BZWLyNdG"},"outputs":[{"name":"stdout","output_type":"stream","text":["1280\n"]}],"source":["num_classes=21\n","in_channels=3\n","depth=4\n","option=\"small\"\n","size=256\n","\n","# ArcFace Hyperparameters\n","arcFace_config = {\n","    \"s\": 30.0,  # scale (The scale parameter changes the shape of the logits. The higher the scale, the more peaky the logits vector becomes.)\n","    \"m\": 0.50,  # margin (margin results in a bigger separation of classes in your training set)\n","    \"ls_eps\": 0.0,\n","    \"easy_margin\": False\n","}\n","\n","model = HappyWhaleModel(\"tf_efficientnet_b0_ns\", 512, num_class=20, arcface_config=arcFace_config)\n","#ResNet(num_classes, in_channels, depth, option, size, use_arcface=False)"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4436,"status":"ok","timestamp":1650141238285,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"wt1byr2QyNdH","outputId":"086b86a2-fc3c-4c50-9bfb-c9442186de5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["                image  individual_id\n","0  000a8f2d5c316a.jpg             12\n","1  001001f099519f.jpg              2\n","2  00144776eb476d.jpg             12\n","3  0024057bbc89a4.jpg              1\n","4  0028f6fa123686.jpg              9\n","Dataset size : 1000\n","Size of validation set : 160\n","Size of test set : 200\n","Size of train set : 640\n"]}],"source":["data_manager = DataManager(data_csv, dataFolderPath, batch_size,\n","                test_percentage, val_percentage, verbose=True)\n","\n","model_trainer = ModelTrainTestManager(model=model,\n","                                        data_manager = data_manager,\n","                                        loss_fn=nn.CrossEntropyLoss(),\n","                                        optimizer_factory=optimizer_factory,\n","                                        exp_name = exp_name ,\n","                                        learning_rate=learning_rate,\n","                                        use_cuda=True)"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":727},"executionInfo":{"elapsed":223,"status":"error","timestamp":1650141238464,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"i6yvatE5yNdJ","outputId":"8c9b18f3-d852-42a1-934f-cda08f82b514"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1 of 10\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/64 [00:01<?, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.95 GiB total capacity; 2.05 GiB already allocated; 15.88 MiB free; 2.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/julien/Documents/3A-Sherbrooke/Forage de donnee/projet/HappyWhale.ipynb Cell 12'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/julien/Documents/3A-Sherbrooke/Forage%20de%20donnee/projet/HappyWhale.ipynb#ch0000011?line=0'>1</a>\u001b[0m epoch\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/julien/Documents/3A-Sherbrooke/Forage%20de%20donnee/projet/HappyWhale.ipynb#ch0000011?line=1'>2</a>\u001b[0m model_trainer\u001b[39m.\u001b[39;49mtrain(epoch)\n","File \u001b[0;32m~/Documents/3A-Sherbrooke/Forage de donnee/projet/src/ModelTrainTestManager.py:112\u001b[0m, in \u001b[0;36mModelTrainTestManager.train\u001b[0;34m(self, num_epochs, start_epoch, metric_values)\u001b[0m\n\u001b[1;32m    <a href='file:///home/julien/Documents/3A-Sherbrooke/Forage%20de%20donnee/projet/src/ModelTrainTestManager.py?line=108'>109</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    <a href='file:///home/julien/Documents/3A-Sherbrooke/Forage%20de%20donnee/projet/src/ModelTrainTestManager.py?line=110'>111</a>\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/julien/Documents/3A-Sherbrooke/Forage%20de%20donnee/projet/src/ModelTrainTestManager.py?line=111'>112</a>\u001b[0m train_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(train_inputs, train_labels)\n\u001b[1;32m    <a href='file:///home/julien/Documents/3A-Sherbrooke/Forage%20de%20donnee/projet/src/ModelTrainTestManager.py?line=112'>113</a>\u001b[0m \u001b[39m# computes loss using loss function loss_fn\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/julien/Documents/3A-Sherbrooke/Forage%20de%20donnee/projet/src/ModelTrainTestManager.py?line=113'>114</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(train_outputs, train_labels)\n","File \u001b[0;32m~/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Documents/3A-Sherbrooke/Forage de donnee/projet/src/Models/ArcFace.py:31\u001b[0m, in \u001b[0;36mHappyWhaleModel.forward\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m     <a href='file:///home/julien/Documents/3A-Sherbrooke/Forage%20de%20donnee/projet/src/Models/ArcFace.py?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, images, labels):\n\u001b[0;32m---> <a href='file:///home/julien/Documents/3A-Sherbrooke/Forage%20de%20donnee/projet/src/Models/ArcFace.py?line=30'>31</a>\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(images)\n\u001b[1;32m     <a href='file:///home/julien/Documents/3A-Sherbrooke/Forage%20de%20donnee/projet/src/Models/ArcFace.py?line=31'>32</a>\u001b[0m     pooled_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooling(features)\u001b[39m.\u001b[39mflatten(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='file:///home/julien/Documents/3A-Sherbrooke/Forage%20de%20donnee/projet/src/Models/ArcFace.py?line=32'>33</a>\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(pooled_features)\n","File \u001b[0;32m~/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.venv_ift780/lib/python3.9/site-packages/timm/models/efficientnet.py:520\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/efficientnet.py?line=518'>519</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/efficientnet.py?line=519'>520</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_features(x)\n\u001b[1;32m    <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/efficientnet.py?line=520'>521</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_pool(x)\n\u001b[1;32m    <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/efficientnet.py?line=521'>522</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_rate \u001b[39m>\u001b[39m \u001b[39m0.\u001b[39m:\n","File \u001b[0;32m~/.venv_ift780/lib/python3.9/site-packages/timm/models/efficientnet.py:510\u001b[0m, in \u001b[0;36mEfficientNet.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/efficientnet.py?line=508'>509</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_features\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/efficientnet.py?line=509'>510</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_stem(x)\n\u001b[1;32m    <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/efficientnet.py?line=510'>511</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m    <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/efficientnet.py?line=511'>512</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact1(x)\n","File \u001b[0;32m~/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.venv_ift780/lib/python3.9/site-packages/timm/models/layers/conv2d_same.py:30\u001b[0m, in \u001b[0;36mConv2dSame.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/layers/conv2d_same.py?line=28'>29</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/layers/conv2d_same.py?line=29'>30</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m conv2d_same(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","File \u001b[0;32m~/.venv_ift780/lib/python3.9/site-packages/timm/models/layers/conv2d_same.py:17\u001b[0m, in \u001b[0;36mconv2d_same\u001b[0;34m(x, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/layers/conv2d_same.py?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconv2d_same\u001b[39m(\n\u001b[1;32m     <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/layers/conv2d_same.py?line=13'>14</a>\u001b[0m         x, weight: torch\u001b[39m.\u001b[39mTensor, bias: Optional[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, stride: Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m),\n\u001b[1;32m     <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/layers/conv2d_same.py?line=14'>15</a>\u001b[0m         padding: Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m), dilation: Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), groups: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/layers/conv2d_same.py?line=15'>16</a>\u001b[0m     x \u001b[39m=\u001b[39m pad_same(x, weight\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:], stride, dilation)\n\u001b[0;32m---> <a href='file:///home/julien/.venv_ift780/lib/python3.9/site-packages/timm/models/layers/conv2d_same.py?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(x, weight, bias, stride, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m), dilation, groups)\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.95 GiB total capacity; 2.05 GiB already allocated; 15.88 MiB free; 2.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["epoch=10\n","model_trainer.train(epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1650141238462,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"-OkZSSAoyNdK"},"outputs":[],"source":["model_trainer.evaluate_on_test_set()\n","model_trainer.plot_metrics()"]},{"cell_type":"markdown","metadata":{"id":"4-YJCOZ3yNdK"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"HappyWhale.ipynb","provenance":[]},"interpreter":{"hash":"047959887fc7b401bc223e3aa5310c3a590340a33f47823b1d5749438031b6f9"},"kernelspec":{"display_name":"Python 3.9.7 64-bit ('.venv_ift780': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
