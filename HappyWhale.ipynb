{"cells":[{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3119,"status":"ok","timestamp":1650141231169,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"wGlyYvluyNcx","outputId":"802b5c24-8b87-4664-d6b1-61ce7f19a573"},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["try:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  !mkdir src\n","  !cp -r drive/MyDrive/IFT780/HappyWhale/src/* src/\n","  !mkdir data\n","  !cp -r drive/MyDrive/IFT780/HappyWhale/data/* data/\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","  #%pip install -r requirements.txt\n","\n","# Pour automatiquement recharger les modules externes\n","# voir http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"XTgcMFrryNc7"},"source":["# HappyWhale Challenge - Team WhalePlayed\n","\n","This notebook presents the work done by Gaétan Rey, Julien Levarlet and Timothée Wright, as part of the challenge https://www.kaggle.com/competitions/happy-whale-and-dolphin/overview ."]},{"cell_type":"markdown","metadata":{"id":"FBEpvY25yNc-"},"source":["Imports :"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":2501,"status":"ok","timestamp":1650141233661,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"IF1wvX3dyNc_"},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","from src.ModelTrainTestManager import ModelTrainTestManager, optimizer_setup\n","from src.DataManager import DataManager\n","from src.Models.ResNet import ResNet"]},{"cell_type":"markdown","metadata":{"id":"HPLcrHV6yNdA"},"source":["Parameters for the data :"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1650141233664,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"NHnVDFYkyNdB"},"outputs":[],"source":["data_csv = \"data/five.csv\"\n","dataFolderPath = \"data/five_class\""]},{"cell_type":"markdown","metadata":{"id":"HsWIuBp7yNdC"},"source":["Parameters for the training :"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1650141233666,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"oE-WSe41yNdD"},"outputs":[],"source":["batch_size = 5\n","learning_rate = 0.01\n","optimizer_factory = optimizer_setup(torch.optim.Adam, lr=learning_rate)\n","\n","test_percentage = 0.2\n","val_percentage = 0.2\n","\n","exp_name = \"HappyWhale\""]},{"cell_type":"markdown","metadata":{"id":"VvLH9ra-yNdF"},"source":["Parameters for the model :"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":206,"status":"ok","timestamp":1650141233853,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"cHz9BZWLyNdG"},"outputs":[],"source":["num_classes=5\n","in_channels=3\n","depth=3\n","option=\"small\"\n","size=128\n","\n","model = ResNet(num_classes, in_channels, depth, option, size, use_arcface=False)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4436,"status":"ok","timestamp":1650141238285,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"wt1byr2QyNdH","outputId":"086b86a2-fc3c-4c50-9bfb-c9442186de5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["                image  individual_id\n","0  001001f099519f.jpg              1\n","1  0024057bbc89a4.jpg              0\n","2  0046ceef89b3fc.jpg              2\n","3  005e53b1b6aada.jpg              2\n","4  0106d276033b78.jpg              2\n","Dataset size : 730\n","Size of validation set : 117\n","Size of test set : 146\n","Size of train set : 467\n"]}],"source":["data_manager = DataManager(data_csv, dataFolderPath, batch_size,\n","                test_percentage, val_percentage, verbose=True)\n","\n","model_trainer = ModelTrainTestManager(model=model,\n","                                        data_manager = data_manager,\n","                                        loss_fn=nn.CrossEntropyLoss(),\n","                                        optimizer_factory=optimizer_factory,\n","                                        exp_name = exp_name ,\n","                                        learning_rate=learning_rate,\n","                                        use_cuda=True)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":727},"executionInfo":{"elapsed":223,"status":"error","timestamp":1650141238464,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"i6yvatE5yNdJ","outputId":"8c9b18f3-d852-42a1-934f-cda08f82b514"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/94 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1 of 10\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/94 [00:00<?, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 1.80 GiB already allocated; 0 bytes free; 1.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32mc:\\Users\\gaeta\\OneDrive\\Documents\\Developpement\\Python\\Forage\\Projet\\HappyWhale\\happywhale.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gaeta/OneDrive/Documents/Developpement/Python/Forage/Projet/HappyWhale/happywhale.ipynb#ch0000011?line=0'>1</a>\u001b[0m epoch\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gaeta/OneDrive/Documents/Developpement/Python/Forage/Projet/HappyWhale/happywhale.ipynb#ch0000011?line=1'>2</a>\u001b[0m model_trainer\u001b[39m.\u001b[39;49mtrain(epoch)\n","File \u001b[1;32mc:\\Users\\gaeta\\OneDrive\\Documents\\Developpement\\Python\\Forage\\Projet\\HappyWhale\\src\\ModelTrainTestManager.py:119\u001b[0m, in \u001b[0;36mModelTrainTestManager.train\u001b[1;34m(self, num_epochs, start_epoch, metric_values)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/OneDrive/Documents/Developpement/Python/Forage/Projet/HappyWhale/src/ModelTrainTestManager.py?line=113'>114</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(train_outputs, train_labels)\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/OneDrive/Documents/Developpement/Python/Forage/Projet/HappyWhale/src/ModelTrainTestManager.py?line=114'>115</a>\u001b[0m \u001b[39m# for croosentropy loss softmax and argmax not needed :\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/OneDrive/Documents/Developpement/Python/Forage/Projet/HappyWhale/src/ModelTrainTestManager.py?line=115'>116</a>\u001b[0m \u001b[39m# \"The input is expected to contain raw, unnormalized scores for each class\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/OneDrive/Documents/Developpement/Python/Forage/Projet/HappyWhale/src/ModelTrainTestManager.py?line=116'>117</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/OneDrive/Documents/Developpement/Python/Forage/Projet/HappyWhale/src/ModelTrainTestManager.py?line=117'>118</a>\u001b[0m \u001b[39m# Use autograd to compute the backward pass.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/gaeta/OneDrive/Documents/Developpement/Python/Forage/Projet/HappyWhale/src/ModelTrainTestManager.py?line=118'>119</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/OneDrive/Documents/Developpement/Python/Forage/Projet/HappyWhale/src/ModelTrainTestManager.py?line=120'>121</a>\u001b[0m \u001b[39m# updates the weights using gradient descent                    \u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/OneDrive/Documents/Developpement/Python/Forage/Projet/HappyWhale/src/ModelTrainTestManager.py?line=121'>122</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n","File \u001b[1;32m~\\anaconda3\\envs\\forage\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/anaconda3/envs/forage/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/anaconda3/envs/forage/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/anaconda3/envs/forage/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/anaconda3/envs/forage/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/anaconda3/envs/forage/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/anaconda3/envs/forage/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/gaeta/anaconda3/envs/forage/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n","File \u001b[1;32m~\\anaconda3\\envs\\forage\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/anaconda3/envs/forage/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/anaconda3/envs/forage/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/anaconda3/envs/forage/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/anaconda3/envs/forage/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/gaeta/anaconda3/envs/forage/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/anaconda3/envs/forage/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/gaeta/anaconda3/envs/forage/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 1.80 GiB already allocated; 0 bytes free; 1.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["epoch=10\n","model_trainer.train(epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1650141238462,"user":{"displayName":"Julien LEVARLET","userId":"18246275701255266200"},"user_tz":240},"id":"-OkZSSAoyNdK"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on the test set: 0.193 %\n"]},{"ename":"TypeError","evalue":"plot_metrics() missing 1 required positional argument: 'path'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\gaeta\\OneDrive\\Documents\\Developpement\\Python\\Forage\\Projet\\HappyWhale\\happywhale.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gaeta/OneDrive/Documents/Developpement/Python/Forage/Projet/HappyWhale/happywhale.ipynb#ch0000012?line=0'>1</a>\u001b[0m model_trainer\u001b[39m.\u001b[39mevaluate_on_test_set()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gaeta/OneDrive/Documents/Developpement/Python/Forage/Projet/HappyWhale/happywhale.ipynb#ch0000012?line=1'>2</a>\u001b[0m model_trainer\u001b[39m.\u001b[39;49mplot_metrics()\n","\u001b[1;31mTypeError\u001b[0m: plot_metrics() missing 1 required positional argument: 'path'"]}],"source":["model_trainer.evaluate_on_test_set()\n","model_trainer.plot_metrics()"]},{"cell_type":"markdown","metadata":{"id":"4-YJCOZ3yNdK"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"HappyWhale.ipynb","provenance":[]},"interpreter":{"hash":"047959887fc7b401bc223e3aa5310c3a590340a33f47823b1d5749438031b6f9"},"kernelspec":{"display_name":"Python 3.9.7 64-bit ('.venv_ift780': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
